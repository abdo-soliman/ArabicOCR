{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from segmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageToWords(img):\n",
    "    words = []\n",
    "    lines = breakLines(img)\n",
    "    for line in lines:\n",
    "        line_words = breakWords(line)\n",
    "        line_words.reverse()\n",
    "        for word in line_words:\n",
    "            words.append(word)\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_counts = {\n",
    "    \"ا\" : 0,\n",
    "    \"ب\" : 0,\n",
    "    \"ت\" : 0,\n",
    "    \"ث\" : 0,\n",
    "    \"ج\" : 0,\n",
    "    \"ح\" : 0,\n",
    "    \"خ\" : 0,\n",
    "    \"د\" : 0,\n",
    "    \"ذ\" : 0,\n",
    "    \"ر\" : 0,\n",
    "    \"ز\" : 0,\n",
    "    \"ش\" : 0,\n",
    "    \"س\" : 0,\n",
    "    \"ص\" : 0,\n",
    "    \"ض\" : 0,\n",
    "    \"ط\" : 0,\n",
    "    \"ظ\" : 0,\n",
    "    \"ع\" : 0,\n",
    "    \"غ\" : 0,\n",
    "    \"ف\" : 0,\n",
    "    \"ق\" : 0,\n",
    "    \"ك\" : 0,\n",
    "    \"ل\" : 0,\n",
    "    \"م\" : 0,\n",
    "    \"ن\" : 0,\n",
    "    \"ه\" : 0,\n",
    "    \"و\" : 0,\n",
    "    \"ي\" : 0,\n",
    "    \"ﻻ\" : 0,\n",
    "    \"ى\" : 0,\n",
    "    \"ئ\" : 0,\n",
    "    \"ء\" : 0,\n",
    "    \"ؤ\" : 0,\n",
    "    \"ة\" : 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgToDataSet(img_path, text_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_words = imageToWords(img)\n",
    "    with open(text_path) as f:\n",
    "        text = f.readlines()[0]\n",
    "        text_words = text.split()\n",
    "        index = 0\n",
    "        for word in text_words:\n",
    "            if index >= len(img_words):\n",
    "                break\n",
    "            chars = segmenteCharacters(img_words[index])\n",
    "            if len(chars) == len(word):\n",
    "                chars.reverse()\n",
    "                for i in range(len(word)):\n",
    "                    letter = 255*extractTemplate(chars[i])\n",
    "                    if letter.shape[0] < 25 and letter.shape[1] < 25:\n",
    "                        mask = np.zeros((25, 25))\n",
    "\n",
    "                        vertical_start = int((25 - letter.shape[0]) / 2)\n",
    "                        vertical_end = vertical_start + letter.shape[0]\n",
    "                        horizontal_start = int((25 - letter.shape[1]) / 2)\n",
    "                        horizontal_end = horizontal_start + letter.shape[1]\n",
    "\n",
    "                        mask[vertical_start:vertical_end, horizontal_start:horizontal_end] = letter\n",
    "                        cv2.imwrite(\"./DataSets/dataSet/\" + word[i] + \"/\" + word[i] + \"_\" + str(letter_counts[word[i]]) + \".png\", mask)\n",
    "                        letter_counts[word[i]] += 1\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 10\n",
    "img_path = \"./DataSets/sample/test\" + str(count) + \"/test\" + str(count) + \".png\"\n",
    "text_path = \"./DataSets/sample/test\" + str(count) + \"/test\" + str(count) + \".txt\"\n",
    "imgToDataSet(img_path, text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_path = \"./DataSets/sens.png\"\n",
    "img = cv2.imread(img_path)\n",
    "lines = breakLines(img)\n",
    "line_0 = breakWords(lines[0])\n",
    "line_1 = breakWords(lines[1])\n",
    "line_2 = breakWords(lines[2])\n",
    "words = []\n",
    "for word in line_0:\n",
    "    words.append(word)\n",
    "for word in line_1:\n",
    "    words.append(word)\n",
    "for word in line_2:\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = (cv2.imread(\"./templates/sen_template.png\", 0) / 255).astype(np.uint8)\n",
    "sen2 = (cv2.imread(\"./templates/sen_middle_template.png\", 0) / 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(skeleton, template, threshold=0.4, start=False):\n",
    "    if template.shape[0] > skeleton.shape[0] or template.shape[1] > skeleton.shape[1]:\n",
    "        return 0\n",
    "\n",
    "    img_rgb = cv2.cvtColor((255 * skeleton).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    w, h = template.shape[::-1]\n",
    "    res = cv2.matchTemplate(skeleton.astype(np.uint8),\n",
    "                            template.astype(np.uint8), cv2.TM_CCOEFF_NORMED)\n",
    "    loc = np.where(res >= threshold)\n",
    "\n",
    "    for pt in zip(*loc[::-1]):\n",
    "        cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 1)\n",
    "\n",
    "    return img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareHist(skeleton, template, threshold=0.4):\n",
    "    if template.shape[0] > skeleton.shape[0] or template.shape[1] > skeleton.shape[1]:\n",
    "        return 0\n",
    "\n",
    "    w, h = template.shape[::-1]\n",
    "    res = cv2.matchTemplate(skeleton.astype(np.uint8),\n",
    "                            template.astype(np.uint8), cv2.TM_CCOEFF_NORMED)\n",
    "    result = (res >= threshold) + 0\n",
    "    \n",
    "    show_images([result])\n",
    "    senHist = getHist(template)\n",
    "    wordHist = getHist(result)\n",
    "\n",
    "    maybe = []\n",
    "    for i in range(len(wordHist)-9):\n",
    "        distance = np.sum(abs(wordHist[i:i+10] - senHist))\n",
    "        if distance < 5:\n",
    "            maybe.append((i, distance))\n",
    "\n",
    "#     print(maybe)\n",
    "    minDistance = 5\n",
    "    splitter = None\n",
    "    for window in maybe:\n",
    "        if window[1] < minDistance:\n",
    "            minDistance = window[1]\n",
    "            splitter = window[0]\n",
    "\n",
    "    bgr_skeleton = cv2.cvtColor((255 * (skeleton + 0)).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    if splitter != None:\n",
    "        cv2.line(bgr_skeleton, (splitter, 0), (splitter, bgr_skeleton.shape[1]), (255, 0, 0), 1)\n",
    "        cv2.line(bgr_skeleton, (splitter+9, 0), (splitter+9, bgr_skeleton.shape[1]), (255, 0, 0), 1)\n",
    "\n",
    "    return bgr_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDots(word):\n",
    "    skeleton = toSkeleton(word)\n",
    "    skeleton = skeleton.astype(np.uint8)\n",
    "\n",
    "    #find contours\n",
    "    ctrs, hier = cv2.findContours(skeleton.copy(), cv2.RETR_EXTERNAL, \n",
    "    cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # #sort contours\n",
    "    sorted_ctrs = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "\n",
    "    dots = []\n",
    "    for i, ctr in enumerate(sorted_ctrs):\n",
    "        x, y, w, h = cv2.boundingRect(ctr)\n",
    "        if w < 5 and h < 5:\n",
    "            skeleton[y:y+h, x:x+w] = 0\n",
    "            dots.append((x, w))\n",
    "            \n",
    "    return [skeleton, dots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeight(skeleton, index):\n",
    "    indecies = np.where(skeleton[:, index] == 1)[0]\n",
    "    if len(indecies) > 0:\n",
    "        return abs(indecies.min() - indecies.max())\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchSen(word):\n",
    "    skeleton, dots = removeDots(word)\n",
    "    \n",
    "    uppers = []\n",
    "    for i in range(skeleton.shape[1]):\n",
    "        column = skeleton[:, i]\n",
    "        if np.sum(column) != 0:\n",
    "            uppers.append(skeleton.shape[0] - np.where(skeleton[:, i] == 1)[0].min())\n",
    "        else:\n",
    "            uppers.append(skeleton.shape[0])\n",
    "    \n",
    "    diff = np.diff(uppers)\n",
    "    peaks = []\n",
    "    for i in range(len(diff)-11):\n",
    "        is_peak = False\n",
    "        possible_peaks = []\n",
    "        for j in range(11):\n",
    "            if diff[i+j-1] > 0:\n",
    "                is_peak = True\n",
    "            if diff[i+j-1] < 0 and is_peak and getHeight(skeleton, i+j-1) <= 4:\n",
    "                is_peak = False\n",
    "                possible_peaks.append(i+j-1)\n",
    "\n",
    "        valid_window = True\n",
    "        if len(possible_peaks) > 2:\n",
    "            for i in range(possible_peaks[0], possible_peaks[-1]):\n",
    "                if np.sum(skeleton[:, i]) <= 0:\n",
    "                    valid_window = False\n",
    "                    break\n",
    "            \n",
    "            if valid_window:\n",
    "                for peak in possible_peaks:\n",
    "                    peaks.append(peak)\n",
    "\n",
    "    peaks = list(set(peaks))\n",
    "    mod_peaks = []\n",
    "    if len(peaks) % 3 != 0:\n",
    "        for peak in peaks:\n",
    "            valid = True\n",
    "            for dot in dots:\n",
    "                if peak >= dot[0] and peak <= dot[0]+dot[1]:\n",
    "                    valid = False\n",
    "\n",
    "            if valid:\n",
    "                mod_peaks.append(peak)   \n",
    "    else:\n",
    "        mod_peaks = peaks\n",
    "    \n",
    "    mod_peaks.sort()\n",
    "    sens = []\n",
    "    for i in range(0, len(mod_peaks)-2, 3):\n",
    "        if i+3 <= len(mod_peaks):\n",
    "            sens.append(mod_peaks[i:i+3])\n",
    "\n",
    "    splitters = []\n",
    "    for sen in sens:\n",
    "        splitters.append(sen[0]-2)\n",
    "        splitters.append(sen[-1]+2)\n",
    "    \n",
    "    return splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmenteCharacters(word, to_skeleton=True, debug=False):\n",
    "    full_skeleton = toSkeleton(word) if to_skeleton else word\n",
    "    skeleton = toSkeleton(word) if to_skeleton else word\n",
    "    base_line = getBaseLine(skeleton)\n",
    "    splitters = np.array([]).astype(np.uint8)\n",
    "\n",
    "    first_sep = matchFirstCharacter(word)\n",
    "    if first_sep >= word.shape[1]:\n",
    "        return [full_skeleton]\n",
    "\n",
    "    if first_sep != 0:\n",
    "        splitters = np.append(splitters, first_sep)\n",
    "        word = word[:, first_sep:]\n",
    "        skeleton = toSkeleton(word)\n",
    "\n",
    "    full_hist = getHist(skeleton)\n",
    "    hist = getHist(skeleton[0:base_line-1, :])\n",
    "    diff = np.diff(hist)\n",
    "\n",
    "    # add splitters at the start of region that doesn't contain pixels\n",
    "    empty_space_splitters = addEmptySpaceSep(full_hist) + first_sep\n",
    "    splitters = np.append(np.array(splitters), empty_space_splitters)\n",
    "    \n",
    "    # add sen and shen splitters\n",
    "    sen_splitters = np.array(matchSen(word)) + first_sep\n",
    "    splitters = np.append(splitters, sen_splitters)\n",
    "    \n",
    "    # add splitter at point if you were above base line and return back to baseline\n",
    "    base_line_splitters = []\n",
    "    for i in range(1, len(diff)):\n",
    "        if diff[i-1] != 0 and diff[i] == 0 and hist[i] == 0:\n",
    "            for j in range(0, len(sen_splitters), 2):\n",
    "                if not ((i + first_sep) >= sen_splitters[j] and (i + first_sep) <= sen_splitters[j+1]):\n",
    "                    if abs((i + first_sep) - sen_splitters[j]) > 3 and abs((i + first_sep) - sen_splitters[j+1]) > 3:\n",
    "                        base_line_splitters.append(i + first_sep)\n",
    "\n",
    "    splitters = np.append(splitters, base_line_splitters).astype(np.uint8)\n",
    "    splitters.sort()    # sort splitters in ascending order\n",
    "    \n",
    "    if len(splitters) == 0:\n",
    "        return [full_skeleton]\n",
    "\n",
    "    start = 2 if len(\n",
    "        splitters) > 1 and splitters[1] - splitters[0] <= 3 and splitters[1] in empty_space_splitters else 1\n",
    "    mod_splitters = np.array(\n",
    "        [splitters[1] if start == 2 else splitters[0]]).astype(np.uint8)\n",
    "    real_splitters = [splitters[i] for i in range(\n",
    "        start, len(splitters)) if splitters[i] - splitters[i-1] > 3]\n",
    "    mod_splitters = np.append(mod_splitters, real_splitters).astype(np.uint8)\n",
    "    \n",
    "    if len(mod_splitters) == 0:\n",
    "        return [full_skeleton]\n",
    "\n",
    "    # remove separators with no characters between them\n",
    "    non_character_filtered = []\n",
    "    for i in range(len(mod_splitters)-1):\n",
    "        sub_hist = full_hist[mod_splitters[i] -\n",
    "                             first_sep:mod_splitters[i+1]-first_sep]\n",
    "        if np.sum((sub_hist > 1) + 0) > 0:\n",
    "            non_character_filtered.append(mod_splitters[i])\n",
    "    if np.sum(full_hist[mod_splitters[-1]-first_sep:]) > 0:\n",
    "        non_character_filtered.append(mod_splitters[-1])\n",
    "\n",
    "    if len(non_character_filtered) == 0:\n",
    "        return [full_skeleton]\n",
    "\n",
    "    # remove fake alef at the beginning\n",
    "    start = 1 if isFakeAlef(skeleton, non_character_filtered[0]) else 0\n",
    "    fake_alef_splitter_filtered = np.array(\n",
    "        non_character_filtered[start:]).astype(np.uint8)\n",
    "\n",
    "    if len(fake_alef_splitter_filtered) == 0:\n",
    "        return [full_skeleton]\n",
    "\n",
    "    if debug:\n",
    "        bgr_skeleton = cv2.cvtColor(\n",
    "            (255 * full_skeleton).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        for splitter in fake_alef_splitter_filtered:\n",
    "            cv2.line(bgr_skeleton, (splitter, 0), (splitter,\n",
    "                                                   bgr_skeleton.shape[1]), (255, 0, 0), 1)\n",
    "\n",
    "        return bgr_skeleton\n",
    "    else:\n",
    "        characters = []\n",
    "        characters.append(full_skeleton[:, 0:fake_alef_splitter_filtered[0]])\n",
    "        for i in range(len(fake_alef_splitter_filtered)-1):\n",
    "            characters.append(\n",
    "                full_skeleton[:, fake_alef_splitter_filtered[i]:fake_alef_splitter_filtered[i+1]])\n",
    "        characters.append(full_skeleton[:, fake_alef_splitter_filtered[-1]:])\n",
    "\n",
    "        return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word = words[0]\n",
    "chars = segmenteCharacters(word)\n",
    "show_images([np.concatenate((chars[0], chars[1]), axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in words:\n",
    "#     show_images([word, cv2.Canny(word,100,200)])\n",
    "#     compareHist(toSkeleton(word), sen1)\n",
    "#     show_images([toSkeleton(word), matchSen(toSkeleton(word))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = toSkeleton(words[0])\n",
    "nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(skeleton, None, None, None, 8, cv2.CV_32S)\n",
    "sizes = stats[1:, -1] #get CC_STAT_AREA component\n",
    "img2 = np.zeros((labels.shape), np.uint8)\n",
    "\n",
    "for i in range(0, nlabels - 1):\n",
    "    if sizes[i] >= 50:   #filter small dotted regions\n",
    "        img2[labels == i + 1] = 255\n",
    "\n",
    "res = cv2.bitwise_not(img2)\n",
    "show_images([skeleton, res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
